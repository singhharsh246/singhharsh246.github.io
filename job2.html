<!DOCTYPE html>
<html>
  <head>
    <title>Product Analyst at Megashots Internet PVT LTD (getMega)</title>
  </head>
  <body>
    <h1>Product Analyst at Megashots Internet PVT LTD (getMega)</h1>
    <h3>Bengaluru, KT, India</h3>
    <h3>Sept 2019 - July 2021</h3>
    <p>I joined getMega in 2019, when they had 30 employees and in 2 years, the company grew to upto 200 employees. The revenue was up from $100 a month in 2019 to $140,000 by the end of July 2021. I worked as the lead product analyst for trivia section of the games on the app</p>

    <h2>Project 1: Question difficulty personalisation</h2>
    <p>Assigning the right difficulty level to questions is key to the success of any trivia game. Players will quickly lose interest if the questions are too easy or too difficult. To address this problem, we developed an algorithm to assign difficulty scores to questions in our trivia game. The algorithm was highly effective and resulted in a 10-fold increase in user engagement. Here's how we did it:<br><br>

    1. Gather data on user engagement: We started by collecting data on user engagement with the trivia game. We measured the time users spent playing the game, how many questions they answered correctly, and how many questions they attempted. This data helped us identify patterns in user behavior and determine which questions were causing users to disengage.<br><br>

    2. Analyze data: We analyzed the data to identify which questions were too easy, too difficult, or just right. We looked for patterns in the questions that users answered incorrectly or avoided attempting.<br><br>

    3. Develop a scoring system: Based on our analysis, we developed a scoring system that assigned a difficulty score to each question. We assigned higher difficulty scores to questions that users consistently answered incorrectly and lower difficulty scores to questions that were answered correctly by most users. This ensured that the questions were appropriately challenging and kept users engaged.<br><br>

    4. Implement scoring system: Once we had developed the scoring system, we implemented it in the trivia game. Each question was now assigned a difficulty score based on our scoring system.<br><br>

    5. Test and refine: We tested the new scoring system and collected data on user engagement. We analyzed the data to determine if the new scoring system was effective in improving user engagement. We refined the scoring system as necessary based on the data.<br><br>

    6. Monitor and adjust: We monitored user engagement with the trivia game over time and adjusted the scoring system as necessary to continue improving user engagement.<br><br>

By using this algorithm to assign difficulty scores to questions in the trivia game, we were able to significantly improve user engagement. Users were more likely to spend longer periods of time playing the game and attempting more questions, resulting in a 10-fold increase in user engagement. Our new scoring system was well-received by users and helped make our trivia game one of the most popular games in the app store.</p>


    <h2>Project 2: Fraud detection model</h2>
    <p>Fraud detection is a crucial aspect of online gaming, and identifying fraudulent behavior can be challenging. To address this problem, I developed a fraud detection model that analyzed player behavior during gameplay in our trivia games. The model was highly effective and resulted in a 350% decrease in churn in some of our trivia games. Here's how we did it:<br><br>

    1. Data collection: We started by collecting data on player behavior during gameplay. We looked at various metrics such as the number of questions answered correctly, the time taken to answer questions, and the frequency of gameplay.<br><br>

    2. Data preprocessing: The collected data was preprocessed to remove any irrelevant or noisy data. We also used various techniques such as scaling and normalization to prepare the data for analysis.<br><br>

    3. Feature engineering: We then created new features based on the data we collected. These features included metrics such as the player's accuracy rate, average response time, and time spent playing the game.<br><br>

    4. Model development: Once we had prepared the data, we developed a machine learning model to detect fraudulent behavior. We used various algorithms such as logistic regression, decision trees, and random forests to create the model.<br><br>

    5. Model testing and validation: We tested and validated the model on a separate dataset to ensure that it was effective in detecting fraudulent behavior.<br><br>

    6. Model deployment: Once we were satisfied with the model's performance, we deployed it in our trivia games to detect fraudulent behavior in real-time.<br><br>

    7. Continuous monitoring and improvement: We continuously monitored the model's performance and made improvements as necessary to ensure that it remained effective in detecting fraudulent behavior.<br><br>

Thanks to this fraud detection model, we were able to identify and prevent fraudulent behavior in our trivia games, leading to a 350% decrease in churn. The model was highly effective in identifying fraudulent behavior in real-time, allowing us to take immediate action to prevent further fraud. Additionally, it was well-received by our users who appreciated the added security and protection provided by the model. Overall, this fraud detection model was a significant success for our company, and we continue to use it to protect our games and our users.</p>

    <h2>Project 3: Implementation of skill based rating system</h2>
    <p>When I first joined getMega, I noticed that user engagement was very low for both card and trivia games. To address this issue, I decided to implement a skill-based rating system to increase user engagement. The results were remarkable, leading to a 500% increase in user engagement for both card and trivia games.

Here's how I did it: <br><br>

    1. Data Collection: The first step was to collect data on user behavior and engagement levels. We gathered data on the number of games played, the number of wins and losses, and the time spent playing the games.<br><br>

    2. Skill-based Rating System: Based on the data collected, we created a skill-based rating system. The system ranked players based on their performance and skill level. This meant that players were matched with other players of similar skill levels, which made the game more competitive and engaging.<br><br>

    3. Implementation: Once we had developed the skill-based rating system, we implemented it across all of our card and trivia games.<br><br>

    4. User Feedback: We collected user feedback on the new system and made adjustments as necessary. Users appreciated the added challenge and competitive aspect of the new system, which resulted in increased engagement and longer playing times.<br><br>

    5. Data Analysis: We continuously monitored user engagement levels and analyzed the data to assess the impact of the new system. The results were astonishing, with a 500% increase in user engagement levels for both card and trivia games.<br><br>

The implementation of the skill-based rating system was a game-changer for Trivia Pod. The new system created a more competitive and engaging environment, which led to increased user engagement and longer playing times. The success of the new system also resulted in increased revenue for the company. Overall, the implementation of the skill-based rating system was a major achievement, and I'm proud to have played a key role in its development and implementation.</p>


  <h2>Project 4: A/B testing for implementation of new models</h2>
  <p>When working on the development of different models for Trivia Pod, I understood the importance of implementing an A/B testing experiment to ensure a smooth rollout of the new features. A/B testing is a powerful tool that allows us to compare the performance of different models, and make data-driven decisions on which ones to roll out to the public.

Here's how I designed and implemented the A/B testing experiment:<br><br>

    1. Define the Objectives: The first step was to define the objectives of the A/B testing experiment. We wanted to test the performance of two different models we had developed, and understand how they would impact user engagement and retention.<br><br>

    2. Develop the Experiment Plan: Next, we developed a detailed experiment plan that defined the test groups, the metrics to be tracked, and the duration of the test. We decided to run the experiment for two weeks, with one group of users receiving the new model and the other group receiving the existing model.<br><br>

    3. Implement the Experiment: Once the experiment plan was developed, we implemented the experiment across a random sample of users. We carefully monitored the performance of both models during the test period.<br><br>

    4. Analyze the Results: At the end of the experiment, we analyzed the results and compared the performance of the two models. We looked at key metrics such as user engagement, retention, and revenue generated.<br><br>

    5. Draw Conclusions and Make Recommendations: Based on the results of the A/B testing experiment, we drew conclusions and made recommendations on which model to roll out to the public. We considered factors such as user feedback, model performance, and ease of implementation.<br><br>

The A/B testing experiment was a critical step in the development and rollout of the new models for Trivia Pod. By carefully designing and implementing the experiment, we were able to make data-driven decisions that led to improved user engagement and retention. The experiment also helped us to identify areas for further improvement and optimization. Overall, the A/B testing experiment was a great success, and I'm proud to have played a key role in its design and implementation.</p>

  </body>
</html>
